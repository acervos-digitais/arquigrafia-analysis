{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object Detection / Captioning / Scene Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "\n",
    "from os import listdir, makedirs, path\n",
    "\n",
    "from PIL import Image as PImage\n",
    "\n",
    "from dominant_colors import get_dominant_colors, resize_PIL\n",
    "\n",
    "IMAGES_IN_PATH = \"../../imgs/arquigrafia\"\n",
    "\n",
    "OUT_PATH = \"./metadata/objects\"\n",
    "makedirs(OUT_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OBJS = {\n",
    "  \"awning\": 0.3,\n",
    "  \"balcony\": 0.2,\n",
    "  \"chair\": 0.2,\n",
    "  \"chimney\": 0.33,\n",
    "  \"door portico\": 0.3,\n",
    "  \"door\": 0.2,\n",
    "  \"masonry\": 0.3,\n",
    "  \"overhang\": 0.3,\n",
    "  \"painting\": 0.4,\n",
    "  \"person\": 0.3,\n",
    "  \"hand rail\": 0.3,\n",
    "  \"pedestrian ramp\": 0.3,\n",
    "  \"sculpture\": 0.4,\n",
    "  \"stairs\": 0.2,\n",
    "  \"steps\": 0.2,\n",
    "  \"support arch\": 0.2,\n",
    "  \"support column\": 0.3,\n",
    "  \"table\": 0.2,\n",
    "  \"brick\": 0.2,\n",
    "  \"concrete wall\": 0.2,\n",
    "  \"stone wall\": 0.2,\n",
    "  \"window\": 0.2,\n",
    "}\n",
    "\n",
    "OBJS_LABELS = sorted(OBJS.keys())\n",
    "OBJS_THOLDS = [OBJS[k] for k in OBJS_LABELS]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer, pipeline\n",
    "\n",
    "CAP_MODEL_NAME = \"openbmb/MiniCPM-V-2\"\n",
    "CAP_MODEL_REV = \"187851962daa9b63072d40ec802f597b71bff532\"\n",
    "\n",
    "CAP_COND = [\n",
    "  {'role': 'user', 'content': \"The following image is a picture taken in Brazil.\"},\n",
    "  {'role': 'user', 'content': \"Give a short description of the image.\"},\n",
    "  {'role': 'user', 'content': \"Don't mention sports or winter.\"},\n",
    "]\n",
    "\n",
    "CAP_MODEL = {\n",
    "  \"model\": AutoModel.from_pretrained(CAP_MODEL_NAME, revision=CAP_MODEL_REV, trust_remote_code=True, torch_dtype=torch.bfloat16).to(\"cuda\", dtype=torch.bfloat16),\n",
    "  \"pre\": AutoTokenizer.from_pretrained(CAP_MODEL_NAME, revision=CAP_MODEL_REV, trust_remote_code=True),\n",
    "  \"chat\": CAP_COND\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENPT_MODEL_NAME = \"Helsinki-NLP/opus-mt-tc-big-en-pt\"\n",
    "ENPT_PIPELINE = pipeline(model=ENPT_MODEL_NAME, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_caption(img, model):\n",
    "  caption, _, _ = model[\"model\"].chat(\n",
    "    image=img,\n",
    "    msgs=model[\"chat\"],\n",
    "    max_length=32,\n",
    "    context=None,\n",
    "    tokenizer=model[\"pre\"],\n",
    "    sampling=True,\n",
    "    temperature=0.1\n",
    "  )\n",
    "  caption = caption[:caption.find(\".\") + 1]\n",
    "  caption = caption[:caption.find(\", possibly\")]\n",
    "  return caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Owlv2Processor, Owlv2ForObjectDetection\n",
    "\n",
    "OBJ_TARGET_SIZE = torch.Tensor([500, 500])\n",
    "OBJ_MODEL = \"google/owlv2-base-patch16-ensemble\"\n",
    "\n",
    "obj_model = Owlv2ForObjectDetection.from_pretrained(OBJ_MODEL).to(\"cuda\")\n",
    "obj_processor = Owlv2Processor.from_pretrained(OBJ_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_px_to_pct(box, img_w, img_h, model_dims):\n",
    "  scale_factor = torch.tensor([max(img_w, img_h) / img_w , max(img_w, img_h) / img_h])\n",
    "  return [round(x, 4) for x in (box.cpu().reshape(2, -1) / model_dims * scale_factor).reshape(-1).tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_object_detection(img, obj_labels, obj_tholds):\n",
    "  input = obj_processor(text=obj_labels, images=img, return_tensors=\"pt\").to(\"cuda\")\n",
    "  with torch.no_grad():\n",
    "    obj_out = obj_model(**input)\n",
    "\n",
    "  res = obj_processor.post_process_object_detection(outputs=obj_out, target_sizes=[OBJ_TARGET_SIZE])\n",
    "  slbs = zip(res[0][\"scores\"], res[0][\"labels\"], res[0][\"boxes\"])\n",
    "  iw, ih = img.size\n",
    "\n",
    "  # filter if box \"too large\" or \"too small\"\n",
    "  def good_thold_and_size(s, l, b):\n",
    "    box_pct = box_px_to_pct(b, iw, ih, OBJ_TARGET_SIZE)\n",
    "    box_width = box_pct[2] - box_pct[0]\n",
    "    box_height = box_pct[3] - box_pct[1]\n",
    "    good_width = box_width > 0.05 and box_width < 0.5 \n",
    "    good_height = box_height > 0.05 and box_height < 0.5\n",
    "    return good_width and good_height and s > obj_tholds[l.item()]\n",
    "\n",
    "  detected_objs = [{\"score\": s, \"label\": obj_labels[l.item()], \"box\": box_px_to_pct(b, iw, ih, OBJ_TARGET_SIZE)} for s,l,b in slbs if good_thold_and_size(s, l, b)]\n",
    "\n",
    "  # only keep the box with highest score per object\n",
    "  detected_objs_boxes = {}\n",
    "  high_score = {}\n",
    "\n",
    "  for o in detected_objs:\n",
    "    ol = o[\"label\"]\n",
    "    if (ol not in detected_objs_boxes) or (o[\"score\"] > high_score[ol]):\n",
    "      detected_objs_boxes[ol] = o[\"box\"]\n",
    "      high_score[ol] = o[\"score\"]\n",
    "\n",
    "  return detected_objs_boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "input_files = sorted([f for f in listdir(IMAGES_IN_PATH) if f.endswith(\"jpg\")])\n",
    "\n",
    "for io_file in input_files[:100]:\n",
    "  input_file_path = path.join(IMAGES_IN_PATH, io_file)\n",
    "  output_file_path = path.join(OUT_PATH, io_file.replace(\".jpg\", \".json\"))\n",
    "\n",
    "  if path.isfile(output_file_path):\n",
    "    continue\n",
    "\n",
    "  print(IMAGES_IN_PATH, io_file)\n",
    "\n",
    "  image = PImage.open(input_file_path).convert(\"RGB\")\n",
    "\n",
    "  rgb_by_count, rgb_by_hls = get_dominant_colors(resize_PIL(image))\n",
    "\n",
    "  image_data = {}\n",
    "  image_data[\"caption\"] = {}\n",
    "  image_data[\"caption\"][\"en\"] = run_caption(image, CAP_MODEL)\n",
    "  image_data[\"caption\"][\"pt\"] = ENPT_PIPELINE(image_data[\"caption\"][\"en\"])[0][\"translation_text\"]\n",
    "  image_data[\"boxes\"] = run_object_detection(image, OBJS_LABELS, OBJS_THOLDS)\n",
    "\n",
    "  image_data[\"dominant_color\"] = {\n",
    "    \"by_count\": [int(v) for v in rgb_by_count[0]],\n",
    "    \"by_hue\": [int(v) for v in rgb_by_hls[0]]\n",
    "  }\n",
    "\n",
    "  with open(output_file_path, \"w\", encoding=\"utf-8\") as of:\n",
    "    json.dump(image_data, of, sort_keys=True, separators=(',',':'), ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-Process: Create output json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from os import listdir, path\n",
    "\n",
    "from dominant_colors import hls_order_from_rgb255\n",
    "\n",
    "CAPTIONS_PATH = \"./metadata/objects\"\n",
    "OBJECTS_DB_FILE_PATH = \"./metadata/objects.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename -> image info\n",
    "img_data = {}\n",
    "\n",
    "# obj name -> image name\n",
    "obj_data = {}\n",
    "\n",
    "# image name -> color order key\n",
    "color_key = {}\n",
    "\n",
    "input_files = sorted([f for f in listdir(CAPTIONS_PATH) if f.endswith(\"json\")])\n",
    "\n",
    "for io_file in input_files:\n",
    "  input_file_path = path.join(CAPTIONS_PATH, io_file)\n",
    "  with open(input_file_path, \"r\", encoding=\"utf8\") as f:\n",
    "    id = int(io_file.replace(\".json\", \"\"))\n",
    "    img_data[id] = json.load(f)\n",
    "\n",
    "    for l in img_data[id][\"boxes\"].keys():\n",
    "      obj_data[l] = obj_data.get(l, []) + [id]\n",
    "\n",
    "    color_key[id] = hls_order_from_rgb255(img_data[id][\"dominant_color\"][\"by_hue\"])\n",
    "\n",
    "# order each object's file list by color order\n",
    "for k in obj_data.keys():\n",
    "  obj_data[k] = sorted(obj_data[k], key=lambda x: color_key[x])\n",
    "\n",
    "out_data = {\n",
    "  \"objects\": obj_data,\n",
    "  \"images\": img_data,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(OBJECTS_DB_FILE_PATH, \"w\", encoding=\"utf8\") as f:\n",
    "  json.dump(out_data, f, separators=(',',':'), sort_keys=True, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST: boxes from JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from os import path\n",
    "from PIL import Image as PImage, ImageDraw as PImageDraw\n",
    "\n",
    "OBJECTS_DB_FILE_PATH = \"./metadata/objects.json\"\n",
    "IMAGES_PATH = \"../../imgs/arquigrafia\"\n",
    "\n",
    "with open(OBJECTS_DB_FILE_PATH, \"r\") as f:\n",
    "  json_data = json.load(f)\n",
    "  img_data = json_data[\"images\"]\n",
    "  obj_data = json_data[\"objects\"]\n",
    "\n",
    "for id, d in list(img_data.items())[:3]:\n",
    "  img_path = path.join(IMAGES_PATH, f\"{id}.jpg\")\n",
    "  img = PImage.open(img_path).convert(\"RGBA\")\n",
    "  iw,ih = img.size\n",
    "  draw = PImageDraw.Draw(img)\n",
    "  for _, (x0,y0,x1,y1) in d[\"boxes\"].items():\n",
    "    draw.rectangle(((x0*iw, y0*ih), (x1*iw, y1*ih)), outline=(255, 0, 0))\n",
    "  print(list(d[\"boxes\"].keys()), \"\\n\", d[\"caption\"])\n",
    "  display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST: EN -> PT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PHRASES = [\n",
    "  \"I like to eat rice.\",\n",
    "  \"Tom tried to stab me.\",\n",
    "  \"He has been to Hawaii several times.\",\n",
    "  \"The image features a white house with black trim, windows on the front and side walls.\",\n",
    "  \"This image features a modern, open-concept living space with an eye-catching staircase and various furniture pieces.\",\n",
    "  \"The image depicts an interior space with a staircase, furniture such as chairs and tables.\",\n",
    "  \"The image showcases a modern building with glass walls, concrete stairs leading to it and greenery surrounding the area.\",\n",
    "  \"The image shows a view through glass panes, revealing indoor furniture and plants outside.\",\n",
    "  \"The image is of a modern building with large windows and columns.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "ENPT_MODEL_NAME = \"Helsinki-NLP/opus-mt-tc-big-en-pt\"\n",
    "ENPT_PIPELINE = pipeline(model=ENPT_MODEL_NAME, device=\"cuda\")\n",
    "\n",
    "for p in PHRASES:\n",
    "  print(ENPT_PIPELINE(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST: Dominant Color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from os import listdir, path\n",
    "from PIL import Image as PImage\n",
    "\n",
    "from dominant_colors import get_dominant_colors, resize_PIL\n",
    "\n",
    "IMAGES_IN_PATH = \"../../imgs/arquigrafia\"\n",
    "INPUT_FILES = sorted([f for f in listdir(IMAGES_IN_PATH) if f.endswith(\"jpg\")])\n",
    "\n",
    "io_file = INPUT_FILES[0]\n",
    "input_file_path = path.join(IMAGES_IN_PATH, io_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = PImage.open(input_file_path).convert(\"RGB\")\n",
    "image_s = resize_PIL(image)\n",
    "rgb_by_count, rgb_by_hls = get_dominant_colors(image_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iw, ih = [(d // 2) * 2 for d in image_s.size]\n",
    "image_shape = (ih, iw, 3)\n",
    "ppc = int(ih * iw / len(rgb_by_count))\n",
    "\n",
    "count_np_image = np.array([ppc * [c] for c in rgb_by_count]).reshape(image_shape)\n",
    "hls_np_image = np.array([ppc * [c] for c in rgb_by_hls]).reshape(image_shape)\n",
    "\n",
    "display(image_s)\n",
    "display(PImage.fromarray(count_np_image))\n",
    "display(PImage.fromarray(hls_np_image))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
